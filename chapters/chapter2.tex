\chapter{Gimbal Control}
\label{chapter2}
\graphicspath{{figures/}{figures/chapter2/}}
Gimbals are widely used device for flying drones with a camera because they stabilize the camera from vibration and movement of the platform. Stabilizing the camera is important in order to take high-quality, smooth videos and photos. For this reason, most of the high-end commercial drones are equipped with a gimbal. For autonomous systems, a gimbal becomes more important because it adds maneuverability to the camera to provide more visual awareness and to keep the object of interest in the camera field of view. There are two cases when the object of interest can be centered on the optical axis. The first is when we know the relative location of the target to the camera. The second is when we know the target location in the image. For many practical applications, it is not feasible to assume that we know the target location in the global frame. Thus, image-based gimbal control is the more practical and realistic way to achieve the goal. In this chapter, we introduce several image-based gimbal control schemes.

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
    	\centering
    	\includegraphics[width=2in]{images/chapter2/gimbal1_frame}
    	\caption{Top view to show the relationship between the body frame and the gimbal-1 frame}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.5\textwidth}
    	\centering
    	\includegraphics[width=2in]{images/chapter2/gimbal_frame}
    	\caption{Side view to show the relationship between the gimbal-1 frame and the gimbal frame}
    \end{subfigure}
    \caption{Frames of interest: body frame, gimbal-1 frame, and gimbal frame}
    \label{gimbal_frame}
\end{figure*}

\section{Angle Commanding Gimbal Control}
\subsection{Coordinate Frame Convention and Projective Camera Geometry}
Before giving a detailed explanation of the gimbal control algorithms, we describe the coordinate frame convention and projective camera geometry. Note that the coordinate frame convention follows the convention from chapter 13 of \cite{beard2012small}. We assume that the origins of the gimbal and camera frames are at the center of mass (COM) of the UAV. This is a reasonable assumption because the distance between the camera, gimbal and UAV COM is negligible compared to the distance between the UAV COM and the target. Also, note that all coordinate frames and the direction of rotation follow the right-hand rule. There are four frames of interests: the body frame of the UAV denoted by $\mathcal{F}^b=(x^{b}, y^{b}, z^{b})$, the gimbal-1 frame denoted by $\mathcal{F}^{g1}=(x^{g1}, y^{g1}, z^{g1})$, the gimbal frame denoted by $\mathcal{F}^{g}=(x^{g}, y^{g}, z^{g})$, and the camera frame denoted by $\mathcal{F}^{c}=(x^{c}, y^{c}, z^{c})$. The gimbal-1 frame can be obtained by rotating $\mathcal{F}^b$ about $z^{b}$ axis by $\alpha_{az}$ which we call the gimbal azimuth angle. The gimbal frame can be obtained by rotating $\mathcal{F}^{g1}$ about $y^{g1}$ by $\alpha_{el}$ which we call the gimbal elevation angle. Note that pointing the gimbal toward the ground is a negative elevation angle (See Fig. \ref{gimbal_frame}). The camera frame is defined so that the $z^{c}$ axis points along the optical axis. Thus, the rotation from body frame to gimbal-1 frame can be expressed as

\begin{equation}
R^{g1}_b(\alpha_{az}) = \begin{bmatrix}
\cos\alpha_{az} & \sin\alpha_{az} & 0 \\
-\sin\alpha_{az} & \cos\alpha_{az} & 0 \\
0 & 0 & 1
\end{bmatrix}.
\label{eq1}
\end{equation}
Similarly, the rotation from gimbal-1 frame to gimbal frame can be expressed as 
\begin{equation}
R^g_{g1}(\alpha_{el})
= \begin{bmatrix}
\cos\alpha_{el} &  0 & -\sin\alpha_{el}\\
0 & 1 & 0 \\
\sin\alpha_{el} & 0 & \cos\alpha_{el}
\end{bmatrix}.
\label{eq2}
\end{equation}
Combining equations \ref{eq1} and \ref{eq2}, we get the rotation from the body frame $\mathcal{F}^b$ to gimbal frame $\mathcal{F}^{g}$ as
\begin{equation}
R^{g}_b(\alpha_{az}, \alpha_{el}) = R^g_{g1}(\alpha_{az})R^{g1}_b(\alpha_{el}) =
\begin{bmatrix}
\cos\alpha_{el}\cos\alpha_{az} & \cos\alpha_{el}\sin\alpha_{az} & -\sin\alpha_{el} \\
-\sin\alpha_{az} & \cos\alpha_{az} & 0 \\
\sin\alpha_{el}\cos\alpha_{az} & \sin\alpha_{el}\sin\alpha_{az} & \cos\alpha_{el}
\end{bmatrix}.
\label{eq3}
\end{equation}

\begin{figure}[htbp]
	\centering
	\includegraphics[width = 3.5in]{images/chapter2/camera_frame}
	\caption{Camera frame and visual aid for projective camera model}
	%
	\label{camera_frame}
\end{figure}

The camera frame, often called the optical frame, follows the common computer vision convention: $x$-axis is directed to the right, $y$-axis is directed down and the $z$-axis is directed out from the camera optical sensor (See Fig. \ref{camera_frame}). Thus, the rotation from the gimbal frame to the camera frame is fixed and is given by 
\begin{equation}
R^{c}_g =
\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{bmatrix}.
\label{eq4}
\end{equation}

The basic idea of the projective camera model is that the 3D world is projected onto a 2D image plane that is orthogonal to $z^c$ axis with distance being the focal length $\lambda$ from the origin of the optical axis (See Fig. \ref{camera_frame}). Because of this projection, we lose the distance information to the target which introduces many problems in computer vision and estimation. 
The focal length of the camera is the distance between the optical sensor and the lens, and it can be obtained through camera calibration. Since common camera calibration gives the focal length in the units of pixels, the camera's field of view angle ($M$) can also be obtained as
\begin{equation}
M=2\tan^{-1}\bigg(\frac{\epsilon_{max}}{\lambda}\bigg)
\end{equation}
where $\epsilon_{max}$ is the maximum pixel value from the center of the image assuming the image is square. 

\subsection{Derivation}
Let the unit vector from the origin of the camera frame to the target (i.e. the line of sight vector or LOS vector) in the camera frame be defined as 
\begin{equation}
\hat{\ell}^c=\frac{1}{L}
\begin{pmatrix}
\epsilon_x \\
\epsilon_y \\
\lambda
\end{pmatrix}
=\frac{1}{\sqrt{\epsilon_x^2+\epsilon_y^2+\lambda^2}}
\begin{pmatrix}
\epsilon_x \\
\epsilon_y \\
\lambda
\end{pmatrix}
=\begin{pmatrix}
\hat{\ell}_x^c \\
\hat{\ell}_y^c \\
\hat{\ell}_z^c
\end{pmatrix}.
\label{unitlos}
\end{equation}
The angle commanding gimbal control algorithm computes the gimbal azimuth and elevation angles that would make the optical axis align with the unit LOS vector. By transforming the unit LOS vector (\ref{unitlos}) into the body frame, we get the desired optical axis as
\begin{equation}
\hat{\ell}^b=R_{g}^b R_{c}^g\hat{\ell}^c.
\label{unitbodylos}
\end{equation}
By equating the equation (\ref{unitbodylos}) to the unit optical axis vector transformed into the body frame, we can compute the desired azimuth and elevation commands as
\begin{align}
\hat{\ell}^b&=\begin{pmatrix}
\hat{\ell}_x^b \\
\hat{\ell}_y^b \\
\hat{\ell}_z^b
\end{pmatrix}=R_{g}^b(\alpha_{az}^d, \alpha_{el}^d) R_{c}^g\begin{pmatrix}
0 \\ 0 \\ 1
\end{pmatrix}
\\&=\begin{bmatrix}
\cos\alpha_{el}^d\cos\alpha_{az}^d & -\sin\alpha_{az}^d & \sin\alpha_{el}^d\cos\alpha_{az}^d \\
\cos\alpha_{el}^d\sin\alpha_{az}^d & \cos\alpha_{az}^d & \sin\alpha_{el}^d\sin\alpha_{az}^d \\
-\sin\alpha_{el}^d & 0 & \cos\alpha_{el}^d
\end{bmatrix}
\begin{bmatrix}
0 & 0 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0
\end{bmatrix}
\begin{pmatrix}
0 \\ 0 \\ 1
\end{pmatrix}
\\&=\begin{pmatrix}
\cos\alpha_{el}^d\cos\alpha_{az}^d \\
\cos\alpha_{el}^d\sin\alpha_{az}^d \\
-\sin\alpha_{el}^d
\end{pmatrix}.
\end{align}
Solving for $\alpha_{az}^d$ and $\alpha_{el}^d$ as 
\begin{equation}
\alpha_{az}^d=\tan^{-1}
\begin{pmatrix}
\frac{\hat{\ell}_y^b}{\hat{\ell}_x^b}
\end{pmatrix}
\end{equation}
\begin{equation}
\alpha_{el}^d=-\sin^{-1}
\begin{pmatrix}
\hat{\ell}_z^b
\end{pmatrix},
\end{equation}
results in the gimbal commands that place the object of interest at the center of the image.

\subsection{Hardware Result}
\begin{figure}[htbp]
	\centering
	\includegraphics[width = 3in]{images/chapter2/gimbal_webcam.jpg}
	\caption{Prototype hardware to test the gimbal control algorithm}
	\label{gimbal_webcam}
\end{figure}

The gimbal control algorithm has been implemented and tested with simple hardware (see Figure \ref{gimbal_webcam}). The major hardware component includes the BaseCam SimpleBGC 32-bit gimbal controller, a webcam, DYS 3-axis GoPro gimbal, and the AS5048B magnetic rotary encoder attached to each rotating axis to measure the gimbal angles. Note that in this hardware experiment, the roll axis of the gimbal is always commanded to be zero to model a pan-tilt gimbal. The focal length of the camera is known through the camera calibration process and the target pixel location is given by the color detection algorithm implemented in OpenCV \cite{itseez2015opencv}. The objective of the gimbal control is to keep the target at the center of the camera field of view (see Figure \ref{gimbal_result}. A video of the results can be found at \href{https://www.youtube.com/watch?v=OZ0Mg8AoAzk}{https://www.youtube.com/watch?v=OZ0Mg8AoAzk}). 

\begin{figure*}[htbp]
	\centering
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=6cm]{images/chapter2/gimbal_0s.png}
		\caption{when $t=0s$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=6cm]{images/chapter2/gimbal_10s.png}
		\caption{when $t=10s$}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=6cm]{images/chapter2/gimbal_20s.png}
		\caption{when $t=20s$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=6cm]{images/chapter2/gimbal_30s.png}
		\caption{when $t=30s$}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=6cm]{images/chapter2/gimbal_40s.png}
		\caption{when $t=40s$}
	\end{subfigure}	
	\caption{Hardware demonstration. The gimbal control algorithm is keeping the target object at the center of the camera field of view.}
	\label{gimbal_result}
\end{figure*}


\section{Angular Velocity Commanding Gimbal Control}
In the previous section, a relatively simple gimbal control algorithm is presented. One disadvantage of that algorithm is that it does not take the camera velocity into consideration. When the gimbal is mounted on a stationary platform, the angle commanding controller can perform without any performance degradation. However, if the gimbal is mounted on a moving UAV, then taking the camera velocity into account in the controller becomes important. The work in \cite{Hurak2012} addresses this issue and derives an algorithm that overcomes the issue. This angular velocity commanding gimbal control algorithm is presented briefly in this section because some of the concepts in this section introduces important background for the next section. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width = 3in]{images/chapter2/cross_elevation.pdf}
	\caption{Elevation and cross-elevation axis}
	\label{cross_elevation}
\end{figure}
\subsection{Image Jacobian}
A full derivation of getting Image Jacobian matrix (often called interaction matrix) can be found in \cite{spong2006robot}. Here, we only show the final form of the Image Jacobian in the process of developing the gimbal control algorithm. Let the camera translational and rotational velocities both expressed in the camera frame be defined as
\begin{equation}
\mathbf{v}^c=[v_x^c, v_y^c, v_z^c]^\top
\end{equation}
\begin{equation}
\mathbf{\omega}^c=[\omega_x^c, \omega_y^c, \omega_z^c]^\top
\end{equation}
\begin{equation}
\mathbf{\xi}=[{\mathbf{v}^c}^\top, {\mathbf{\omega}^c}^\top]^\top.
\label{camera_velocity}
\end{equation}
Let $\alpha_{el}$ be the elevation angle, and let $\alpha_{az}$ be the azimuth angle. Also, following the convention in Figure \ref{camera_frame}, 
\begin{equation}
R^{g}_c =
\begin{bmatrix}
0 & 0 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0
\end{bmatrix}.
\end{equation}
Also, let the coordinates of an image feature and its pixel velocity be defined as 
\begin{equation}
\mathbf{s}=[u, w]^\top
\end{equation}
and 
\begin{equation}
\mathbf{\dot{s}}=[\dot{u}, \dot{w}]^\top.
\label{image_feature_velocity}
\end{equation}
The Image Jacobian matrix $L$ at $\mathbf{s}$ is the relationship between the camera velocity in (\ref{camera_velocity}) and the image feature velocity in (\ref{image_feature_velocity}) and can be expressed as 
\begin{equation}
\mathbf{\dot{s}}=L(\mathbf{s},z,\lambda)\mathbf{\xi}
\end{equation}
or more explicitly
\begin{equation}
\begin{bmatrix}
\dot{u} \\ \dot{w}
\end{bmatrix}
=\begin{bmatrix}
-\frac{\lambda}{z} & 0 & \frac{u}{z} & \frac{uw}{\lambda} & -\frac{\lambda^2+u^2}{\lambda} & w \\
0 & -\frac{\lambda}{z} & \frac{w}{z} & \frac{\lambda^2+w^2}{\lambda} & -\frac{uw}{\lambda} & -u
\end{bmatrix}
\begin{bmatrix}
v_x^c \\ v_y^c \\ v_z^c \\
\omega_x^c \\ \omega_y^c \\ \omega_z^c
\end{bmatrix}
\label{image_jacobian}
\end{equation}
where $z$ is the depth along the optical axis to the target, and $\lambda$ is the focal length. Note that $\lambda$ is a fixed parameter once the camera is calibrated. The equation (\ref{image_jacobian}) can be divided into two parts: one for the camera translational velocity and the other for rotational velocity as
\begin{align}
\mathbf{\dot{s}}&=\begin{bmatrix}
-\frac{\lambda}{z} & 0 & \frac{u}{z} \\
0 & -\frac{\lambda}{z} & \frac{w}{z}
\end{bmatrix}\mathbf{v}^c+
\begin{bmatrix}
\frac{uw}{\lambda} & -\frac{\lambda^2+u^2}{\lambda} & w \\
\frac{\lambda^2+w^2}{\lambda} & -\frac{uw}{\lambda} & -u
\end{bmatrix}\mathbf{\omega}^c
\\&=L_v(u,w,z)\mathbf{v}^c+L_{\omega}(u,w)\mathbf{\omega}^c
\label{image_jacobian1}
\end{align}
It is worth noting that only the translational term depends on the target depth $z$. 

\subsection{Feedback Linearization-based Visual Pointing and Tracking}
The control objective is to drive an image feature to the center of the image. The horizontal error can be eliminated by moving two axis gimbal motors attached on azimuth and elevation axis. Moving only the azimuth axis cannot fully compensate for the horizontal error because it only indirectly affects $z^g$ axis which is often referred as cross-elevation axis $cel$ (see Figure \ref{cross_elevation}). Thus, some vertical error is introduced and it needs to be compensated by moving the elevation at next time step. However, the key idea of the controller is that by exploiting $\omega_x^g$ term which is the angular velocity along the optical axis, we can find the azimuth and elevation commands for the same time step that smoothly compensates for the horizontal error without introducing the vertical error. Let the error in the image plane be
\begin{equation}
e(t)=s(t)-s^{ref}
\label{image_feature_error}
\end{equation}
where $s^{ref}=[0, 0]^\top$ to push the image feature to the image center. We would like to drive $e(t)$ to zeros by controlling the elevation angular velocity $\omega_{EL}=\omega_y^g$ and the azimuth angular velocity $\omega_{az}=\frac{\omega_z^g}{\cos \alpha_{el}}$. Re-arranging equation (\ref{image_jacobian1}) gives
\begin{equation}
L_{\omega}(u,w)\mathbf{\omega}^c=\dot{s}-L_v(u,w,z)\mathbf{v}^c.
\label{image_jacobian2}
\end{equation}
Note that we can only command the camera angular rate where the camera linear velocity can be measured by other sensors. The null space of the matrix $L_{\omega}$ can be parameterized as
\begin{equation}
\mathcal{N}\{L_{\omega}\}=\{k\begin{bmatrix}
u & w & \lambda
\end{bmatrix}^\top\}.
\label{nullspace}
\end{equation}
Therefore adding (\ref{nullspace}) to (\ref{image_jacobian2}) gives
\begin{equation}
L_{\omega}(u,w)\mathbf{\omega}^c=\dot{s}-L_v(u,w,z)\mathbf{v}^c+L_{\omega}(u,w)k\begin{bmatrix}
u \\ w \\ \lambda
\end{bmatrix}.
\label{image_jacobian3}
\end{equation}
The null space parametrization means that a rotation about the axis connecting the image feature and the origin of the optical axis does not change the coordinates of the image feature. The left pseudoinverse of $L_{\omega}$ is given by 
\begin{equation}
L_{\omega}^\sharp=\begin{bmatrix}
0 & \frac{\lambda}{\lambda^2+u^2+w^2} \\
-\frac{\lambda}{\lambda^2+u^2+w^2} & 0 \\
\frac{w}{\lambda^2+u^2+w^2} & -\frac{u}{\lambda^2+u^2+w^2}
\end{bmatrix}.
\label{pseudoinverse}
\end{equation}
Multiplying both side of (\ref{image_jacobian3}) by (\ref{pseudoinverse}) yields
\begin{equation}
\mathbf{\omega}^c=\frac{1}{z(\lambda^2+u^2+w^2)}\begin{bmatrix}
\lambda^2 v_y-\lambda v_z w+\lambda \dot{w}z+ku \\
-\lambda^2 v_x+\lambda v_zu-\lambda \dot{u}z+kw \\
-\lambda u v_y+\lambda w v_x-u\dot{w}z+\dot{u}wz+k\lambda
\end{bmatrix}.
\label{angular_rate}
\end{equation}
If we require $\dot{s}^{ref}(t)=-\alpha Is(t)$ where $\alpha$ is a positive value, the actual value of $s(t)$ is expected to be exponentially stable which means it converges to zero. Thus, 
\begin{equation}
\mathbf{\dot{s}}^{ref}=\begin{bmatrix}
\dot{u}^{ref} \\ \dot{w}^{ref}
\end{bmatrix}
=-\alpha \begin{bmatrix}
1 & 0 \\ 0 & 1
\end{bmatrix}
\begin{bmatrix}
u \\ w
\end{bmatrix}
=\begin{bmatrix}
-\alpha u \\ -\alpha w
\end{bmatrix}.
\label{sdot_reference}
\end{equation}
By plugging equation (\ref{sdot_reference}) into (\ref{angular_rate}), we can compute the desired camera angular rate that guarantees the asymptotic stability for the image error as
\begin{equation}
{\mathbf{\omega}^c}^{ref}=\frac{1}{z(\lambda^2+u^2+w^2)}\begin{bmatrix}
\lambda^2 v_y-\lambda v_z w -\lambda \alpha wz+ku \\
-\lambda^2 v_x+\lambda v_zu +\lambda \alpha uz+kw \\
-\lambda u v_y+\lambda w v_x +k\lambda
\end{bmatrix}.
\label{angular_rate_reference}
\end{equation}
This angular rate reference command expressed in the camera frame must be transformed into the gimbal frame in order to command each gimbal axis as
\begin{align}
{\mathbf{\omega}^g}^{ref}&=R_c^g{\mathbf{\omega}^c}^{ref}
=\begin{bmatrix}
{\omega_x^g}^{ref} \\ {\omega_y^g}^{ref} \\ {\omega_z^g}^{ref}
\end{bmatrix}
\\&=\frac{1}{z(\lambda^2+u^2+w^2)}\begin{bmatrix}
-\lambda u v_y+\lambda w v_x +k\lambda \\
\lambda^2 v_y-\lambda v_z w -\lambda \alpha wz+ku \\
-\lambda^2 v_x+\lambda v_zu +\lambda \alpha uz+kw \\
\end{bmatrix}.
\label{angular_rate_ref_elevation}
\end{align}
Unfortunately ${\mathbf{\omega}^c}^{ref}$ contains reference commands for three axes, but there are only two controllable axes, namely azimuth and elevation. This problem can be solved by using the free parameter $k$ to come up with two proper motor commands. The key strategy is to select $k$ so that $\omega_x^g=0$. Accordingly,  
\begin{equation}
k=uv_y-wv_x+\frac{z(\lambda^2+u^2+w^2)}{\lambda}\omega_x^g
\label{k}
\end{equation}
where $\omega_x^g$ must be measured from a sensor such as a MEMS gyro attached to the camera. Substituting for $k$ in ${\omega_y^g}^{ref}$ and ${\omega_z^g}^{ref}$ from the equation (\ref{angular_rate_ref_elevation}) with (\ref{k}) gives
\begin{equation}
\omega_{el}^{ref}={\omega_y^g}^{ref}=\frac{\lambda^2 v_y-\lambda v_z w -\lambda \alpha wz+u^2 v_y-uwv_x}{z(\lambda^2+u^2+w^2)}+\frac{\omega_x^g u}{\lambda}
\end{equation}
\begin{equation}
\omega_{cel}^{ref}={\omega_z^g}^{ref}=\frac{-\lambda^2 v_x+\lambda v_z u+\lambda \alpha uz +uwv_y -w^2 v_x}{z(\lambda^2+u^2+w^2)}+\frac{\omega_x^g w}{\lambda}.
\end{equation}
Using the relationship 
\begin{equation}
\omega_{az}=\frac{1}{\cos\alpha_{el}}\omega_{cel},
\end{equation}
we can compute the desired angular rate on the azimuth axis as
\begin{equation}
\omega_{az}^{ref}=\frac{1}{\cos \alpha_{el}}\bigg(\frac{-\lambda^2 v_x+\lambda v_z u+\lambda \alpha uz +uwv_y -w^2 v_x}{z(\lambda^2+u^2+w^2)}+\frac{\omega_x^g w}{\lambda}\bigg).
\end{equation}
\begin{figure}[htbp]
	\centering
	\includegraphics[width = 5in]{images/chapter2/gp_blockdiagram.pdf}
	\caption{Angular velocity commanding gimbal controller block diagram}
	\label{avcg_blockdiagram}
\end{figure}
The system block diagram of this controller is shown in Figure \ref{avcg_blockdiagram}. The controller presents the ideal control scheme for a two axis inertially stabilized gimbal system, but it also presents some technical challenges such as estimating the depth to the target or estimating the camera translational velocity. The camera translational velocity can be either estimated by using the UAV velocity if available or be treated as a disturbance. Estimating the depth can be handled using a laser range finder if available or geolocation technique. Another option is to use the novel adaptive depth gimbal control algorithm that we introduce in the next section.

\section{Adaptive Depth Gimbal Control}
The gimbal control algorithm in the previous section provides a method to keep a target of interest in the camera's field of view with gimbal azimuth and elevation control. However, the target depth $z$ is required to complete the Jacobian (\ref{image_jacobian}). This section examines a strategy to remove this requirement by estimating the depth online. This controller is derived using Model Reference Adaptive Control (MRAC).

\subsection{Introduction}
Model Reference Adaptive Control (MRAC) is a useful control scheme to stabilize dynamical systems even when there are unknown parameters in the system. A general MRAC closed-loop block diagram can be found in Figure \ref{MRAC_blockdiagram} \cite{Lavretsky2013}. The design of an MRAC system requires a reference model that specifies how the actual system should behave. An adaptive law then adapts the parameters using the error between the reference model and the system output. Finally, the controller computes the command for the plant using the adjusted parameters, the external command, and the system output. 
\begin{figure}[htbp]
	\centering
	\includegraphics[width = 5in]{images/chapter2/MRAC.pdf}
	\caption{An MRAC closed-loop block diagram}
	\label{MRAC_blockdiagram}
\end{figure}
\subsection{Derivation}
Consider again the Image Jacobian matrix
\begin{equation}
\begin{bmatrix}
\dot{u} \\ \dot{w}
\end{bmatrix}
=\begin{bmatrix}
-\frac{\lambda}{z} & 0 & \frac{u}{z} & \frac{uw}{\lambda} & -\frac{\lambda^2+u^2}{\lambda} & w \\
0 & -\frac{\lambda}{z} & \frac{w}{z} & \frac{\lambda^2+w^2}{\lambda} & -\frac{uw}{\lambda} & -u
\end{bmatrix}
\begin{bmatrix}
v_x^c \\ v_y^c \\ v_z^c \\
\omega_x^c \\ \omega_y^c \\ \omega_z^c
\end{bmatrix}\\
\label{image_jacobian4}
\end{equation}
or for simplicity
\begin{equation}
\mathbf{\dot{s}}=L_v(u,w,z)\mathbf{v}^c+L_{\omega}(u,w)\mathbf{\omega}^c.
\end{equation}
Since we desire that the target is pushed to the center of the image plane, the reference model can be set as 
\begin{equation}
\mathbf{\dot{s}}^{ref}=A\mathbf{s}^{ref}
\label{reference_model}
\end{equation}
where $\mathbf{s}=[u, w]^\top$ and $A$ is Hurwitz (i.e. every eigenvalue of $A$ has strictly negative real part). It is clear that the reference model in (\ref{reference_model}) is globally asymptotically stable. From the image jacobian matrix, $\mathbf{\omega}^c$ is the control input, because it is common situation to track the target with gimbal movement rather than platform movement. Thus, equation (\ref{image_jacobian4}) can be manipulated into a more convenient form for MRAC design as 
\begin{align}
\mathbf{\dot{s}}
&=\frac{1}{z}
\begin{bmatrix}
-\lambda v_x^c+uv_z^c \\
-\lambda v_y^c+wv_z^c \\
\end{bmatrix}
+\begin{bmatrix}
\frac{uw}{\lambda} & -\frac{\lambda^2+u^2}{\lambda} & w \\
\frac{\lambda^2+w^2}{\lambda} & -\frac{uw}{\lambda} & -u
\end{bmatrix}
\begin{bmatrix}
\omega_x^c \\ \omega_y^c \\ \omega_z^c
\end{bmatrix}
\\&=\beta\varphi + L_{\omega}U
\label{adaptive_image_jacobian}
\end{align}
where $U$ is the control input and $\beta=\frac{1}{z}$. If $\beta$ were known, then the ideal control input would be
\begin{equation}
U=L_\omega^\sharp(-\beta\varphi+K\mathbf{s})
\end{equation}
where $L_\omega^\sharp$ is the right pseudoinverse of $L_\omega$ and $K$ is a negative definite tuning matrix. Then, the subsequent dynamics would become
\begin{equation}
\mathbf{\dot{s}}=K\mathbf{s}
\end{equation}
which is globally asymptotically stable. However, since $\beta$ is an unknown quantity and needs to be adapted, a more realistic control input is
\begin{equation}
U=L_\omega^\sharp(-\hat{\beta}\varphi+K\mathbf{s})
\end{equation}
where $\hat{\beta}$ is the estimate of $\beta$. Plugging this control input into the equation (\ref{adaptive_image_jacobian}) gives
\begin{align}
\mathbf{\dot{s}}&=\beta\varphi-\hat{\beta}\varphi+K\mathbf{s}
\\&= \tilde{\beta}\varphi+K\mathbf{s}
\end{align}
where $\tilde{\beta}=\beta-\hat{\beta}$. Let the error be defined as 
\begin{equation}
\mathbf{e}=\mathbf{s}-\mathbf{s}^{ref}.
\end{equation}
Taking derivative and equating $K$ to $A$ gives
\begin{align}
\dot{\mathbf{e}}&=\dot{\mathbf{s}}-\dot{\mathbf{s}}^{ref}
\\&=\tilde{\beta}\varphi+K\mathbf{s}-A\mathbf{s}^{ref}
\\&=\tilde{\beta}\varphi+A\mathbf{e}.
\end{align}
Using the variables of interest, a Lyapunov function candidate can be constructed as 
\begin{equation}
V=\frac{1}{2}\mathbf{e}^\top\mathbf{e}+\frac{1}{2\gamma_\beta}\tilde{\beta}^2.
\end{equation}
Taking the derivative yields
\begin{align}
\dot{V}&=\mathbf{e}^\top\dot{\mathbf{e}}+\frac{1}{\gamma_\beta}\tilde{\beta}\dot{\tilde{\beta}}
\\&=\mathbf{e}^\top(\tilde{\beta}\varphi+A\mathbf{e})+\frac{1}{\gamma_\beta}\tilde{\beta}\dot{\tilde{\beta}}.
\label{vdot2}
\end{align}
Assuming that the inverse depth $\beta$ is constant or slowly moving ($\dot{\beta}=0$, $\dot{\tilde{\beta}}=-\dot{\hat{\beta}}$), the above equation becomes
\begin{align}
\dot{V}&=\mathbf{e}^\top A\mathbf{e}+\tilde{\beta}(\mathbf{e}^\top\varphi-\frac{\dot{\hat{\beta}}}{\gamma_\beta}).
\end{align}
Since $\dot{\hat{\beta}}$ is a design parameter, its value can be selected as 
\begin{equation}
\dot{\hat{\beta}}=\gamma_\beta\mathbf{e}^\top\varphi.
\end{equation}
Then, the equation (\ref{vdot2}) becomes
\begin{equation}
\dot{V}=\mathbf{e}^\top A\mathbf{e}.
\end{equation}
Asymptotic stability of the system follows from the fact that A is Hurwitz.
\subsection{Simulation}
First, the reference model $\mathbf{s}^{ref}$, in equation (\ref{reference_model}) is selected as $[0, 0]$ over time. The objective of the MRAC controller is to drive the actual image dynamics with unknown quantity $z$ as close as it can to the reference model. In simulation, a square image with 800 pixel width and height is used. The camera field of view is set to 60 degrees and one pixel is set to one millimeter. Thus, we can obtain the focal length and image width in metric units. The initial image coordinates are randomly selected and these coordinates are expected to converge to the origin as the reference model. In reality, the depth $z$ is a varying quantity because of the relative movement between the camera and the target. However, in this simulation $z$ is initialized to an arbitrary value between 30 and 300, and is changed randomly with increment $\pm2$ every second. In order to simulate the translational velocity of the UAV, $v_x^c$ and $v_y^c$ are initialized to zeros and changed with increment $\pm2$ every other second. $v_z^c$ is kept zero assuming that the camera is not directly flying into nor away from the target. Figure \ref{adaptive_result} shows that the controller keeps the target in the center of the image which is the primary objective. Figure \ref{adaptive_command} presents the angular velocity commands computed by the adaptive depth gimbal controller and the depth estimation. Based on Figure \ref{adaptive_command}, the estimation of the uncertain parameter seems to converge to its true value. However, note that this is rather a special case. In normal cases, only uniform boundness of the uncertain parameters is guaranteed \cite{Lavretsky2013}. 
\begin{figure*}[htbp]
	\centering
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{images/chapter2/u_adaptive}
		\caption{Reference image coordinate $u_{ref}$ and the system output $u$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{images/chapter2/w_adaptive}
		\caption{Reference image coordinate $w_{ref}$ and the system output $w$}
	\end{subfigure}
	\caption{The simulation result for the adaptive depth gimbal control. The system output $u$ and $w$ are converging to the reference model output $u_{ref}$ and $w_{ref}$.}
	\label{adaptive_result}
\end{figure*}

\begin{figure*}[htbp]
	\centering
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{images/chapter2/angular_velocity_adaptive}
		\caption{Angular velocity commands of the controller}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{images/chapter2/z_adaptive}
		\caption{Online depth estimation, $\hat{z}$}
	\end{subfigure}
	\caption{Angular velocity commands of the adaptive depth gimbal controller. Note that only two commands are used, since it is a pan-tilt gimbal. The depth $z$ estimate using MRAC.}
	\label{adaptive_command}
\end{figure*}

In addition, this gimbal control is simulated on a multirotor equipped with a pan-tilt gimbal. The multirotor is moving and suppose that its translational velocity is available. Also, the target is moving on the ground. The adaptive depth gimbal controller takes the pixel location of the target from a camera at a $30Hz$ frame rate, and its goal is to push the target to the center of the image. The simulated multirotor and camera view are presented in Figure \ref{uav_adaptive}. Additional information regarding the simulation can be found in Figure \ref{uav_adaptive_additional}. These plots are rather typical for MRAC: meeting the control objective without a guarantee that the uncertain parameter will converge to its true value. 

\begin{figure*}[htbp]
	\centering
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=4.5cm]{images/chapter2/uav_adaptive_0s}
		\caption{Multirotor at $t=0s$. The blue star indicates the desired multirotor position for the position PID controller for moving camera platform.}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=4.5cm]{images/chapter2/camera_adaptive_0s}
		\caption{Camera view at $t=0s$}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=4.5cm]{images/chapter2/uav_adaptive_60s}
		\caption{Multirotor at $t=60s$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=4.5cm]{images/chapter2/camera_adaptive_60s}
		\caption{Camera view at $t=60s$}
	\end{subfigure}	
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=4.5cm]{images/chapter2/uav_adaptive_120s}
		\caption{Multirotor at $t=120s$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=4.5cm]{images/chapter2/camera_adaptive_120s}
		\caption{Camera view at $t=120s$}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=4.5cm]{images/chapter2/uav_adaptive_180s}
		\caption{Multirotor at $t=180s$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=4.5cm]{images/chapter2/camera_adaptive_180s}
		\caption{Camera view at $t=180s$}
	\end{subfigure}					
	\caption{Multirotor simulation with camera view. The gimbal pointing objective is well achieved.}
	\label{uav_adaptive}
\end{figure*}

\begin{figure*}[htbp]
	\centering
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{images/chapter2/uav_z}
		\caption{Depth $z$ and its estimate $\hat{z}$. Uncertain parameter converging to true value is not guaranteed.}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{images/chapter2/uav_gimbal_command}
		\caption{Angular velocity gimbal azimuth and elevation commands}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/chapter2/uav_pixel}
		\caption{Pixel value $u$ and $w$. They are maintained around the center of the image.}
	\end{subfigure}	
	\caption{Uncertain parameter estimation, gimbal angular velocity commands from the controller, and where target lies in the image.}
	\label{uav_adaptive_additional}
\end{figure*}

\subsection{Hardware}
The adaptive depth gimbal controller is tested on the custom pan-tilt gimbal shown in Figure \ref{gimbal}. The gimbal consists of a uEye UI-3250-LE-C-HQ camera, BaseCam SimpleBGC 32-bit gimbal controller, CUI AMT203 ABS SPI encoder, Quanum 4008 precision brushless gimbal motor, and 3D printed housing (See Figure \ref{gimbal_parts}). The system block diagram can be found in Figure \ref{gimbal_blockdiagram}. The Arduino acts as a communication bridge by receiving the current azimuth and elevation angle from the encoder and passes them to the onboard computer. Also, it passes the angular velocity commands from the onboard computer to the gimbal controller. A tracking algorithm running on the onboard computer receives an image from the camera and outputs the pixel location of the target. The onboard computer also runs the controller that computes the gimbal angular velocity and passes it to the Arduino.
\begin{figure}[htbp]
	\centering
	\includegraphics[height = 3in]{images/chapter2/gimbal.jpg}
	\caption{A custom pan-tilt camera gimbal}
	\label{gimbal}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\linewidth]{images/chapter2/gimbal_parts.png}
	\caption{A custom pan-tilt camera gimbal}
	\label{gimbal_parts}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\linewidth]{images/chapter2/gimbal_system_blockdiagram}
	\caption{Custom gimbal block diagram}
	\label{gimbal_blockdiagram}
\end{figure}
A series of snapshots that shows the gimbal pointing result with the custom hardware and the adaptive depth gimbal controller are shown in Figure \ref{gimbal_hardware_result}. Also, a video showing the same result can be seen at \href{https://www.youtube.com/watch?v=VMfvQkhD9-o}{https://www.youtube.com/watch?v=VMfvQkhD9-o}. In this particular result, the camera is static which means that $v^c$ is set to zero.
\begin{figure*}[htbp]
	\centering
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=6.5cm]{images/chapter2/gimbal_hardware_1.png}
		\caption{At $t=0s$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=6.5cm]{images/chapter2/gimbal_hardware_2.png}
		\caption{At $t=10s$}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=6.5cm]{images/chapter2/gimbal_hardware_3.png}
		\caption{At $t=20s$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=6.5cm]{images/chapter2/gimbal_hardware_4.png}
		\caption{At $t=30s$}
	\end{subfigure}	
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=6.5cm]{images/chapter2/gimbal_hardware_5.png}
		\caption{At $t=40s$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[height=6.5cm]{images/chapter2/gimbal_hardware_6.png}
		\caption{At $t=50s$}
	\end{subfigure}	
	\caption{Adaptive depth gimbal control result on the custom-built hardware.}
	\label{gimbal_hardware_result}
\end{figure*}

\section{Conclusion}
There are a few controller options to keep a target in the camera field of view using a gimbal. Depending on whether the camera is moving or stationary, either angle commanding gimbal control (ACGC) or angular velocity commanding gimbal control (AVCGC) can be used. The advantage of the ACGC is that it is easy to implement and that it does not require the depth information. The AVCGC is expected to perform better than ACGC for a moving camera since the AVCGC take the moving platform into account. However, the AVCGC requires knowledge of the depth $z$. When there is no way to reliably measure or estimate the depth $z$, the adaptive depth gimbal control (ADGC) can be used. A summary is shown in Table \ref{gimbal_summary}.
\begin{table}[htbp]
	\begin{tabular}{ |c|c|c| } 
		\hline
		ACGC & AVCGC & ADGC \\
		\hline
		Easy to implement & Designed for moving camera & Designed for moving camera \\ 
		Designed for static camera & Requires the knowledge of the depth $z$ & Works without knowing $z$ \\  
		\hline
	\end{tabular}
	\caption{Summary of gimbal control schemes}
	\label{gimbal_summary}
\end{table}